{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RathiKashi_PrashanthiR_Eng-Hin-tensorflow-nmt.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashanthi411/eng-hin-nlp/blob/master/RathiKashi_PrashanthiR_Eng_Hin_tensorflow_nmt.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jO8KYb6yYphB",
        "colab_type": "code",
        "outputId": "1c5b735e-71ec-4a55-bbf9-e2905a041df4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os \n",
        "import io\n",
        "import time\n",
        "!pip3 install indic-nlp-library"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: indic-nlp-library in /usr/local/lib/python3.6/dist-packages (0.6)\n",
            "Requirement already satisfied: morfessor in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (2.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.18.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from indic-nlp-library) (1.0.3)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->indic-nlp-library) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas->indic-nlp-library) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IxmIoNhFh7qo",
        "colab_type": "text"
      },
      "source": [
        "# Downloading the English-hindi dataset\n",
        "\n",
        "The dataset consists of four files: two train files and two test files. The train file consists of one file with English sentences and another with the corresponding Hindi translations, similarly for the test files. \n",
        "\n",
        "The train files contain 84557 translations, out of which we will be using 70,000 for training. The test files contain 1000 translations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qouiinSUbKUd",
        "colab_type": "code",
        "outputId": "a1231240-245c-415d-c675-59dfafa85c7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "!wget \"http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/indic_languages_corpus.tar.gz\""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-06 08:42:40--  http://lotus.kuee.kyoto-u.ac.jp/WAT/indic-multilingual/indic_languages_corpus.tar.gz\n",
            "Resolving lotus.kuee.kyoto-u.ac.jp (lotus.kuee.kyoto-u.ac.jp)... 130.54.208.131\n",
            "Connecting to lotus.kuee.kyoto-u.ac.jp (lotus.kuee.kyoto-u.ac.jp)|130.54.208.131|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 132762852 (127M) [application/x-gzip]\n",
            "Saving to: ‘indic_languages_corpus.tar.gz.1’\n",
            "\n",
            "indic_languages_cor 100%[===================>] 126.61M  25.4MB/s    in 12s     \n",
            "\n",
            "2020-05-06 08:42:53 (10.5 MB/s) - ‘indic_languages_corpus.tar.gz.1’ saved [132762852/132762852]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4AxU0WlbZSx",
        "colab_type": "code",
        "outputId": "0823fd17-a187-47bf-e682-62b7ccc2d904",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tarfile\n",
        "with tarfile.open('indic_languages_corpus.tar.gz', 'r:gz') as tar:\n",
        "    tar.extractall()\n",
        "print(\"done!\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So4-SXJYbtMt",
        "colab_type": "code",
        "outputId": "27de4d68-0c9d-4e8c-b6ba-ccf0c423b11b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%cd indic_languages_corpus/bilingual/hi-en/"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/indic_languages_corpus/bilingual/hi-en\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLiTegnlgyvx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# set the batch size to 64. restricting the total number of sentences to 70000\n",
        "BATCH_SIZE = 64\n",
        "NUM_SENTENCES = 70000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0h_y0wbzcxSs",
        "colab_type": "code",
        "outputId": "789c325a-120c-46f8-937a-d57df75c0ee4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# understanding how the training data looks like\n",
        "f = open('train.hi')\n",
        "w1 = f.readlines()\n",
        "print(len(w1))\n",
        "g = open('train.en')\n",
        "w2 = g.readlines()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "84557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sL_IvHeamlhc",
        "colab_type": "text"
      },
      "source": [
        "# Preprocessing Data\n",
        "\n",
        "Once we have loaded the dataset, we preprocess the data as follows:\n",
        "\n",
        "1. Add a start and end token to each sentence.\n",
        "2. Clean the sentences by removing special characters.\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n",
        "4. Pad each sentence to a maximum length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmsVNqzYdjxo",
        "colab_type": "code",
        "outputId": "192f016b-62f9-47d1-a506-aff3e2fa100a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# strip the input and output of extra unnecessary characters\n",
        "# store all the cleaned input and output sentences into input_sentences[] and output_sentences[]\n",
        "# tokenize the Hindi (target) sentences using the indicNLP libary class and add <sos> (start-of-sentence) and <eos> (end-of-sentence)\n",
        "\n",
        "input_sentences = []\n",
        "output_sentences = []\n",
        "\n",
        "count = 0\n",
        "for line in open(r'train.en', encoding=\"utf-8\"):\n",
        "    count += 1\n",
        "\n",
        "    if count > NUM_SENTENCES:\n",
        "        break\n",
        "\n",
        "    input_sentence = line.rstrip().strip(\"\\n\").strip('-') #we strip the sentence of '\\n' and '-' \n",
        "    input_sentences.append(input_sentence) #store all input sentences in the input sentences list\n",
        "\n",
        "count = 0\n",
        "\n",
        "for line in open(r'train.hi'):\n",
        "    count += 1\n",
        "\n",
        "    if count > NUM_SENTENCES:\n",
        "        break\n",
        "    output_sentence =  line.rstrip().strip(\"\\n\").strip('-') \n",
        "    from indicnlp.tokenize import indic_tokenize  \n",
        "    line = indic_tokenize.trivial_tokenize(output_sentence) #we tokenize the hindi sentences \n",
        "    \n",
        "    output_sentences.append(['<sos>'] + line + ['<eos>']) #append the start and end tags to the tokenised sentences\n",
        "                                                          #each tokenied sentence is stored as a list in output sentences\n",
        "print(type(input_sentences[10]))\n",
        "print(type(output_sentences[10]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2kB9RH2fBE0",
        "colab_type": "code",
        "outputId": "b2da0d21-49a7-4ca7-f647-4e8c4d3e888b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(\"num samples input:\", len(input_sentences))\n",
        "print(\"num samples output:\", len(output_sentences))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "num samples input: 70000\n",
            "num samples output: 70000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkMgkiSnhIQE",
        "colab_type": "code",
        "outputId": "58b25bf2-7681-4292-8632-752ae5589b28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "print(input_sentences[-1])\n",
        "print(output_sentences[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Her face.\n",
            "['<sos>', 'उसका', 'चेहरा', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_PsNxfbSvle",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s):\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # creating a space between a word and the punctuation following it\n",
        "  # eg: \"he is a boy.\" => \"he is a boy .\"\n",
        "\n",
        "  w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "  w = re.sub(r'[\" \"]+', \" \", w)\n",
        "\n",
        "  # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "  w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "\n",
        "  w = w.strip()\n",
        "\n",
        "  # adding a start and an end token to the sentence\n",
        "  # so that the model know when to start and stop predicting.\n",
        "  w = '<sos> ' + w + ' <eos>'\n",
        "  return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NTM7NQ_R8fT",
        "colab_type": "code",
        "outputId": "d0349925-ab49-44bb-b6d7-9371dafaa8b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "for i in range(len(input_sentences)):\n",
        "   input_sentences[i] = preprocess_sentence(input_sentences[i])\n",
        "\n",
        "print(input_sentences[8])\n",
        "print(output_sentences[8])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<sos> i told her we rest on sundays . <eos>\n",
            "['<sos>', 'मैं', 'रविवार', 'को', 'उसे', 'हम', 'बाकी', 'बताया', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OM3aEpCUhxh7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to tokenize, fit the words into numeric sequences and pad them with zeroes up to the size of the largest sentence of that vocabulary\n",
        "# takes as input the input / output vocabulary and the padding type ('pre' / 'post'-- default: post)\n",
        "\n",
        "# inp_lang and targ_lang is of type tokenizer.fit_on_texts; \n",
        "# fit_on_texts of Tokenizer class updates internal vocabulary based on a list of texts. \n",
        "# This method creates the vocabulary index based on word frequency. \n",
        "# Lower integer means more frequent word (often the first few are stop words because they appear a lot).\n",
        "\n",
        "def tokenize(lang, pad): \n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "  \n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "  \n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "  return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgsojarlh81H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function to call the tokenize function to perform tokenizing and padding\n",
        "\n",
        "def load_dataset(inp_lang, targ_lang):\n",
        "  # creating cleaned input, output pairs\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang, 'post')\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang, 'post')\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0FxxPJYFi3cX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(input_sentences, output_sentences)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "# For our project, the max_length_targ and max_length_inp are 69 and 72 respectively.\n",
        "\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "blHpIh2WTR7M",
        "colab_type": "code",
        "outputId": "507d5c6b-d70e-4f0e-f40e-8fa80ac592d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# checking if the input sequences have been obtained and padded properly\n",
        "print(target_tensor[9])\n",
        "print(input_tensor[9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1  47 203  18 203  26  39 553  79  29   5 270   8   2   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "[   1    5  106   62   63  462 6235   21    4   59    8    2    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
            "    0    0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ao4xP-xckEhE",
        "colab_type": "code",
        "outputId": "95d9a0b4-e1bc-4765-b532-829b18fdf9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56000 56000 14000 14000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2gSXFB7DNI9",
        "colab_type": "code",
        "outputId": "a226193b-d0a2-40c2-a6b7-b53f5b4fa910",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# checking if the input sequences have been obtained and padded properly\n",
        "print(input_tensor_val[9])\n",
        "print(target_tensor_val[9])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[  1   6  34  27 120 277   3   2   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            "[  1   9  37  61 188  32   3   2   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yy0Z2a0bl9QB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a function to test if the word to index / index to word mappings have been obtained correctly. \n",
        "# representative output for two sample english and hindi sentences given in the code block below\n",
        "\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t!=0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmJypgD3l3P6",
        "colab_type": "code",
        "outputId": "de99cbc9-e710-4b0e-d488-4ae447c7cda7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <sos>\n",
            "138 ----> yes\n",
            "4 ----> ,\n",
            "6 ----> i\n",
            "486 ----> promise\n",
            "3 ----> .\n",
            "2 ----> <eos>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <sos>\n",
            "362 ----> हां\n",
            "5 ----> ,\n",
            "9 ----> मैं\n",
            "367 ----> वादा\n",
            "121 ----> करता\n",
            "32 ----> हूँ\n",
            "3 ----> .\n",
            "2 ----> <eos>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C5gES2mBo0AW",
        "colab_type": "text"
      },
      "source": [
        "# Creating a tf dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCSWpdefmNaS",
        "colab_type": "code",
        "outputId": "8c954282-ae99-453a-9022-44b6f4e60ff9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# BUFFER_SIZE stores the number of training points\n",
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "\n",
        "# BATCH_SIZE is set to 64. Training and gradient descent happens in batches of 64\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# the number of batches in one epoch (also, the number of steps during training, when we go batch by batch)\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "\n",
        "# the length of the embedded vector\n",
        "embedding_dim = 256\n",
        "\n",
        "# no of GRUs\n",
        "units = 1024 \n",
        "\n",
        "# getting the size of the input and output vocabularies.\n",
        "vocab_inp_size = len(inp_lang.word_index)+1 \n",
        "vocab_tar_size = len(targ_lang.word_index)+1\n",
        "\n",
        "# now, we shuffle the dataset and split it into batches of 64\n",
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True) # the remainder after splitting by 64 are dropped\n",
        "\n",
        "print(BUFFER_SIZE)\n",
        "print(BUFFER_SIZE//64)\n",
        "print(steps_per_epoch)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56000\n",
            "875\n",
            "875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2_KlgAMm-ZK",
        "colab_type": "code",
        "outputId": "95fe5672-61fc-4d6e-9e22-80a386b88870",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# to understand the shape of an input batch\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 72]), TensorShape([64, 69]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IJ35HtdpH9P",
        "colab_type": "text"
      },
      "source": [
        "# **Encoder-Decoder model with attention**\n",
        "\n",
        "The encoder model consists of an embedding layer, a GRU layer with 1024 units.\n",
        "\n",
        "The decoder model consists of an attention layer, a embedding layer, a GRU layer and a dense layer.\n",
        "\n",
        "The attention model consists of three dense layers.\n",
        "\n",
        "The model is as follows (with GRUs instead of LSTMs):\n",
        "\n",
        "\n",
        "<img src=\"https://i.ytimg.com/vi/p3jmVkUMMuw/maxresdefault.jpg\" width=\"500\" alt=\"attention mechanism\">\n",
        "\n",
        "We gratefully acknowledge the [tensorflow tutorial](https://tensorflow.org/tutorials/text/nmt_with_attention) and the  [stackabuse tutorial](https://stackabuse.com/python-for-nlp-neural-machine-translation-with-seq2seq-in-keras/) for the creation of these networks.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yyyfnUqqCSJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_sz = batch_sz # set batch size\n",
        "    self.enc_units = enc_units # set the number of GRU units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # set the embedding layer using the input's vocabulary size and the embedding dimension (which is set to 256)\n",
        "    self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform') # define the GRU layer\n",
        "\n",
        "  def call(self, x, hidden): # this function is invoked when the function encoder is called with an input and an initialised hidden layer\n",
        "    x = self.embedding(x)\n",
        "    output, state = self.gru(x, initial_state = hidden) # pass input x into the GRU layer\n",
        "    return output, state # function returns the encoder output and the hidden state\n",
        "\n",
        "\n",
        "  def initialize_hidden_state(self): #intialise hidden layer to all zeroes (for determining the shape)\n",
        "    return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bo28Qbfa0rIX",
        "colab_type": "code",
        "outputId": "23c65d72-808b-4314-f206-5ae71ad5c55e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE) # create an Encoder class object\n",
        "\n",
        "# sample input to get a sense of the shapes.\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 72, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLc-pUfI04Lu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# a class defined for the attention layer\n",
        "# returns attention weights and context vector.\n",
        "\n",
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self, units):\n",
        "    super(BahdanauAttention, self).__init__()\n",
        "    self.W1 = tf.keras.layers.Dense(units) # fully-connected dense layer-1\n",
        "    self.W2 = tf.keras.layers.Dense(units) # fully-connected dense layer-2\n",
        "    self.V = tf.keras.layers.Dense(1) # fully-connected dense layer-3\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state shape == (batch_size, hidden size)\n",
        "    # query_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "    # values shape == (batch_size, max_len, hidden size)\n",
        "    # we are doing this to broadcast addition along the time axis to calculate the score\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score shape == (batch_size, max_length, 1)\n",
        "    # we get 1 at the last axis because we are applying score to self.V\n",
        "    # the shape of the tensor before applying self.V is (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights shape == (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    # context_vector shape after sum == (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4KgsowW89lf",
        "colab_type": "code",
        "outputId": "11fa7530-3fb0-4dff-d6d4-ff599ae8c234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "attention_layer = BahdanauAttention(10) # create an attention layer object\n",
        "attention_result, attention_weights = attention_layer(sample_hidden, sample_output) # pass sample encoder output and hidden layer to get a sense of the shape of the output of the attention layer.\n",
        "\n",
        "print(\"Attention result shape: (batch size, units) {}\".format(attention_result.shape))\n",
        "print(\"Attention weights shape: (batch_size, sequence_length, 1) {}\".format(attention_weights.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Attention result shape: (batch size, units) (64, 1024)\n",
            "Attention weights shape: (batch_size, sequence_length, 1) (64, 72, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UEMbTmGW9VT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_sz = batch_sz # batch_size which is defined as 64\n",
        "    self.dec_units = dec_units # the number of decoder GRU units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) # defining an embedding layer for the target language output. \n",
        "    self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform') # GRU layer\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    # used for attention\n",
        "    self.attention = BahdanauAttention(self.dec_units)\n",
        "\n",
        "  def call(self, x, hidden, enc_output):\n",
        "    # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "    context_vector, attention_weights = self.attention(hidden, enc_output) # getting the context vector and the attention weights from the attention layer\n",
        "\n",
        "    # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n",
        "    x = self.embedding(x) # creating an embedding layer for the target output\n",
        "\n",
        "    # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n",
        "    x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "\n",
        "    # passing the concatenated vector to the GRU\n",
        "    output, state = self.gru(x)\n",
        "\n",
        "    # output shape == (batch_size * 1, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape == (batch_size, vocab)\n",
        "    x = self.fc(output) # pass the output through the dense layer\n",
        "\n",
        "    return x, state, attention_weights # return decoder output, decoder state and attention weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1wvr3Ie9X81",
        "colab_type": "code",
        "outputId": "4680f813-30e0-4a37-aa57-1c188411c278",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
        "                                      sample_hidden, sample_output)\n",
        "\n",
        "print ('Decoder output shape: (batch_size, vocab size) {}'.format(sample_decoder_output.shape))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 22224)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R1O69JbYG1D1",
        "colab_type": "text"
      },
      "source": [
        "# Defining the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erDnU7mw9acE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True, reduction='none') #Loss function is categorical crossentropy\n",
        "\n",
        "def loss_function(real, pred):\n",
        "  mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "  loss_ = loss_object(real, pred)\n",
        "\n",
        "  mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "  loss_ *= mask\n",
        "\n",
        "  return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ryWGXER9goc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 encoder=encoder,\n",
        "                                 decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dEniteeTHbjH",
        "colab_type": "text"
      },
      "source": [
        "# Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBBj4-9U9r1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "  loss = 0\n",
        "\n",
        "  with tf.GradientTape() as tape:\n",
        "    enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<sos>']] * BATCH_SIZE, 1)\n",
        "\n",
        "    # Teacher forcing - feeding the target as the next input\n",
        "    for t in range(1, targ.shape[1]):\n",
        "      # passing enc_output to the decoder\n",
        "      predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n",
        "\n",
        "      loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "      # using teacher forcing\n",
        "      dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "  batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "  variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "  gradients = tape.gradient(loss, variables) \n",
        "\n",
        "  optimizer.apply_gradients(zip(gradients, variables)) # doing gradient descent\n",
        "\n",
        "  return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GF7cJDaHeMkn",
        "colab_type": "code",
        "outputId": "0bf801ac-7527-43eb-c820-64d825081599",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  start = time.time()\n",
        "\n",
        "  enc_hidden = encoder.initialize_hidden_state()\n",
        "  total_loss = 0\n",
        "\n",
        "  for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "    batch_loss = train_step(inp, targ, enc_hidden)\n",
        "    total_loss += batch_loss\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                                   batch,\n",
        "                                                   batch_loss.numpy()))\n",
        "  # saving (checkpoint) the model every 2 epochs\n",
        "  if (epoch + 1) % 2 == 0:\n",
        "    checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "  print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                      total_loss / steps_per_epoch))\n",
        "  print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.3010\n",
            "Epoch 1 Batch 100 Loss 0.8172\n",
            "Epoch 1 Batch 200 Loss 0.8098\n",
            "Epoch 1 Batch 300 Loss 0.7426\n",
            "Epoch 1 Batch 400 Loss 0.6993\n",
            "Epoch 1 Batch 500 Loss 0.7266\n",
            "Epoch 1 Batch 600 Loss 0.5776\n",
            "Epoch 1 Batch 700 Loss 0.5846\n",
            "Epoch 1 Batch 800 Loss 0.7301\n",
            "Epoch 1 Loss 0.7040\n",
            "Time taken for 1 epoch 699.3439862728119 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.5775\n",
            "Epoch 2 Batch 100 Loss 0.6035\n",
            "Epoch 2 Batch 200 Loss 0.5631\n",
            "Epoch 2 Batch 300 Loss 0.5489\n",
            "Epoch 2 Batch 400 Loss 0.5983\n",
            "Epoch 2 Batch 500 Loss 0.5768\n",
            "Epoch 2 Batch 600 Loss 0.5607\n",
            "Epoch 2 Batch 700 Loss 0.6239\n",
            "Epoch 2 Batch 800 Loss 0.5970\n",
            "Epoch 2 Loss 0.5525\n",
            "Time taken for 1 epoch 632.6380219459534 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.4759\n",
            "Epoch 3 Batch 100 Loss 0.4573\n",
            "Epoch 3 Batch 200 Loss 0.4785\n",
            "Epoch 3 Batch 300 Loss 0.4126\n",
            "Epoch 3 Batch 400 Loss 0.4152\n",
            "Epoch 3 Batch 500 Loss 0.5087\n",
            "Epoch 3 Batch 600 Loss 0.4288\n",
            "Epoch 3 Batch 700 Loss 0.4258\n",
            "Epoch 3 Batch 800 Loss 0.3912\n",
            "Epoch 3 Loss 0.4546\n",
            "Time taken for 1 epoch 631.2751507759094 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.3424\n",
            "Epoch 4 Batch 100 Loss 0.3575\n",
            "Epoch 4 Batch 200 Loss 0.4049\n",
            "Epoch 4 Batch 300 Loss 0.3295\n",
            "Epoch 4 Batch 400 Loss 0.3851\n",
            "Epoch 4 Batch 500 Loss 0.4038\n",
            "Epoch 4 Batch 600 Loss 0.3914\n",
            "Epoch 4 Batch 700 Loss 0.3396\n",
            "Epoch 4 Batch 800 Loss 0.3681\n",
            "Epoch 4 Loss 0.3697\n",
            "Time taken for 1 epoch 632.6801497936249 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.3248\n",
            "Epoch 5 Batch 100 Loss 0.2677\n",
            "Epoch 5 Batch 200 Loss 0.2892\n",
            "Epoch 5 Batch 300 Loss 0.2728\n",
            "Epoch 5 Batch 400 Loss 0.3129\n",
            "Epoch 5 Batch 500 Loss 0.3433\n",
            "Epoch 5 Batch 600 Loss 0.2674\n",
            "Epoch 5 Batch 700 Loss 0.2450\n",
            "Epoch 5 Batch 800 Loss 0.3095\n",
            "Epoch 5 Loss 0.2943\n",
            "Time taken for 1 epoch 632.0312643051147 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.2255\n",
            "Epoch 6 Batch 100 Loss 0.2344\n",
            "Epoch 6 Batch 200 Loss 0.2831\n",
            "Epoch 6 Batch 300 Loss 0.1986\n",
            "Epoch 6 Batch 400 Loss 0.2871\n",
            "Epoch 6 Batch 500 Loss 0.2255\n",
            "Epoch 6 Batch 600 Loss 0.2370\n",
            "Epoch 6 Batch 700 Loss 0.2157\n",
            "Epoch 6 Batch 800 Loss 0.2691\n",
            "Epoch 6 Loss 0.2338\n",
            "Time taken for 1 epoch 632.7256405353546 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.2366\n",
            "Epoch 7 Batch 100 Loss 0.1777\n",
            "Epoch 7 Batch 200 Loss 0.1563\n",
            "Epoch 7 Batch 300 Loss 0.2278\n",
            "Epoch 7 Batch 400 Loss 0.1339\n",
            "Epoch 7 Batch 500 Loss 0.2323\n",
            "Epoch 7 Batch 600 Loss 0.2120\n",
            "Epoch 7 Batch 700 Loss 0.2329\n",
            "Epoch 7 Batch 800 Loss 0.1913\n",
            "Epoch 7 Loss 0.1902\n",
            "Time taken for 1 epoch 631.4042444229126 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.1441\n",
            "Epoch 8 Batch 100 Loss 0.1639\n",
            "Epoch 8 Batch 200 Loss 0.1442\n",
            "Epoch 8 Batch 300 Loss 0.1700\n",
            "Epoch 8 Batch 400 Loss 0.1848\n",
            "Epoch 8 Batch 500 Loss 0.1745\n",
            "Epoch 8 Batch 600 Loss 0.1883\n",
            "Epoch 8 Batch 700 Loss 0.1468\n",
            "Epoch 8 Batch 800 Loss 0.1361\n",
            "Epoch 8 Loss 0.1550\n",
            "Time taken for 1 epoch 632.4759912490845 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0990\n",
            "Epoch 9 Batch 100 Loss 0.1034\n",
            "Epoch 9 Batch 200 Loss 0.1323\n",
            "Epoch 9 Batch 300 Loss 0.1297\n",
            "Epoch 9 Batch 400 Loss 0.1124\n",
            "Epoch 9 Batch 500 Loss 0.1218\n",
            "Epoch 9 Batch 600 Loss 0.1212\n",
            "Epoch 9 Batch 700 Loss 0.0919\n",
            "Epoch 9 Batch 800 Loss 0.1378\n",
            "Epoch 9 Loss 0.1250\n",
            "Time taken for 1 epoch 631.6773598194122 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0848\n",
            "Epoch 10 Batch 100 Loss 0.0873\n",
            "Epoch 10 Batch 200 Loss 0.0930\n",
            "Epoch 10 Batch 300 Loss 0.1220\n",
            "Epoch 10 Batch 400 Loss 0.0793\n",
            "Epoch 10 Batch 500 Loss 0.0913\n",
            "Epoch 10 Batch 600 Loss 0.1052\n",
            "Epoch 10 Batch 700 Loss 0.0920\n",
            "Epoch 10 Batch 800 Loss 0.1201\n",
            "Epoch 10 Loss 0.1001\n",
            "Time taken for 1 epoch 633.3948774337769 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZAFPeCr66Wm",
        "colab_type": "text"
      },
      "source": [
        "# Prediction Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPkwfe6E_Csj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "  attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "\n",
        "  sentence = preprocess_sentence(sentence)\n",
        "\n",
        "  inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "  inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs],\n",
        "                                                         maxlen=max_length_inp,\n",
        "                                                         padding='post')\n",
        "  inputs = tf.convert_to_tensor(inputs)\n",
        "\n",
        "  result = ''\n",
        "\n",
        "  hidden = [tf.zeros((1, units))]\n",
        "  enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "  dec_hidden = enc_hidden\n",
        "  dec_input = tf.expand_dims([targ_lang.word_index['<sos>']], 0)\n",
        "\n",
        "  for t in range(max_length_targ):\n",
        "    predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
        "                                                         dec_hidden,\n",
        "                                                         enc_out)\n",
        "    \n",
        "    # pass the encoder output, decoder hidden state(which is initialised to encoder hidden state for the first time and decoder input to the decoder)\n",
        "    # make a prediction and obtain decoder hidden states and attention weights\n",
        "\n",
        "    # storing the attention weights to plot later on\n",
        "    attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "    attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "    result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "    if targ_lang.index_word[predicted_id] == '<eos>':\n",
        "      return result, sentence, attention_plot\n",
        "\n",
        "    # the predicted ID is fed back into the model\n",
        "    dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "  return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpohHxM4MStb",
        "colab_type": "text"
      },
      "source": [
        "NOTE: In order to plot the attention weights upload the Lohit-Devanagari.ttf file onto the /content section of colab. (The ttf file is part of the folder of this submission)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTFmLGfZ6Vg2",
        "colab_type": "code",
        "outputId": "5261cf7d-f207-4490-e5bb-ec42f2f73f4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "% cd \n",
        "% cd/content\n",
        "% ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/root\n",
            "/content\n",
            "\u001b[0m\u001b[01;34mindic_languages_corpus\u001b[0m/        Lohit-Devanagari.ttf\n",
            "indic_languages_corpus.tar.gz  \u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1OX7GPF_KQh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "from matplotlib.font_manager import FontProperties\n",
        "\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "  fig = plt.figure(figsize=(10,10))\n",
        "  ax = fig.add_subplot(1, 1, 1)\n",
        "  ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "  fontdict = {'fontsize': 18}\n",
        "  hindi_font = FontProperties(fname = 'Lohit-Devanagari.ttf')\n",
        "\n",
        "  ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "  ax.set_yticklabels([''] + predicted_sentence, fontproperties=hindi_font, fontsize='18')\n",
        "  # ax.set_yticklabels([''] + predicted_sentence, fontname='Lohit Devanagari')\n",
        "\n",
        "  ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "  \n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exQRehVR_RR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "  result, sentence, attention_plot = evaluate(sentence)\n",
        "\n",
        "  print('Input: %s' % (sentence))\n",
        "  print('Predicted translation: {}'.format(result))\n",
        "  attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n",
        "  plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n",
        "\n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwgSqCLf_Tht",
        "colab_type": "code",
        "outputId": "95d83003-07ab-44ee-b57b-f7a772d6c1ac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.InitializationOnlyStatus at 0x7ff5e71e3780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30uSq1yAHmPI",
        "colab_type": "text"
      },
      "source": [
        "# Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHcnO2JbFrp7",
        "colab_type": "code",
        "outputId": "85585a6b-b87b-4623-b0af-9bc23631b40d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 992
        }
      },
      "source": [
        "print(translate(\"I am hungry\"))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <sos> i am hungry <eos>\n",
            "Predicted translation: मुझे भूख लगी है <eos> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 108 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 112 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 101 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 111 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:214: RuntimeWarning: Glyph 115 missing from current font.\n",
            "  font.set_text(s, 0.0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 101 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 111 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n",
            "/usr/local/lib/python3.6/dist-packages/matplotlib/backends/backend_agg.py:183: RuntimeWarning: Glyph 115 missing from current font.\n",
            "  font.set_text(s, 0, flags=flags)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "मुझे भूख लगी है <eos> \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st0ebCYp4ypN",
        "colab_type": "text"
      },
      "source": [
        "# Pre processing test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QtrOMxOMWkN",
        "colab_type": "code",
        "outputId": "86fc3026-f0cf-4645-a1c8-b45fd463013e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "test_input_sentences = []\n",
        "test_output_sentences = []\n",
        "\n",
        "for line in open(r'test.en', encoding=\"utf-8\"):\n",
        "\n",
        "    test_input_sentence = line.rstrip().strip(\"\\n\").strip('-')\n",
        "    test_input_sentences.append(test_input_sentence)\n",
        "\n",
        "\n",
        "for line in open(r'test.hi'):\n",
        "    test_output_sentence =  line.rstrip().strip(\"\\n\").strip('-')\n",
        "    line = indic_tokenize.trivial_tokenize(test_output_sentence)\n",
        "    \n",
        "    test_output_sentences.append(['<sos>'] + line + ['<eos>'])\n",
        "    \n",
        "print(type(test_input_sentences[90]))\n",
        "print(len(test_output_sentences))\n",
        "print(test_input_sentences[90])\n",
        "print(test_output_sentences[90])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n",
            "1000\n",
            "You're slower than molasses in January.\n",
            "['<sos>', 'आप', 'जनवरी', 'में', 'गुड़', 'की', 'तुलना', 'में', 'धीमी', 'है', '.', '<eos>']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80CWIiiE6xMa",
        "colab_type": "text"
      },
      "source": [
        "# Calculating BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "biOSECxSapD2",
        "colab_type": "code",
        "outputId": "752aff2d-ea31-47dd-d248-a527ea6cba96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "chencherry = SmoothingFunction()\n",
        "\n",
        "references = []\n",
        "candidates = []\n",
        "for i in range(len(test_input_sentences)):\n",
        "  try:\n",
        "    res = translate(test_input_sentences[i]) \n",
        "    references.append(test_output_sentences[i])\n",
        "    candidates.append(indic_tokenize.trivial_tokenize(res))\n",
        "  except:\n",
        "    pass\n",
        "score1 = corpus_bleu(references, candidates, smoothing_function=chencherry.method4)\n",
        "score2 = corpus_bleu(references, candidates)\n",
        "print('BLEU score on test data without smoothing function: ' ,score2)\n",
        "print('BLEU score on test data with smoothing function: ' ,score1)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BLEU score on test data without smoothing function:  0.5375135203209639\n",
            "BLEU score on test data with smoothing function:  0.23162048599659632\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}